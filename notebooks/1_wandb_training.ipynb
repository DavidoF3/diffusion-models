{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Diffusion Model with W&B\n",
    "\n",
    "In this notebooks we take the material from [basics_training_and_inference.ipynb](./basics_training_and_inference.ipynb) and integrate W&B for tracking the development of a diffusion model. More specifically, we add:\n",
    "* Logging of the training loss and metrics\n",
    "* Sampling from the model during training and uploading the samples to W&B\n",
    "* Saving the model checkpoints to W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from utils.wandb_utils import *\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are storing the parameters to be logged to wandb\n",
    "DATA_DIR = Path('../data_train/')\n",
    "SAVE_DIR = Path('../weights/')\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = SimpleNamespace(\n",
    "    # hyperparameters\n",
    "    num_samples = 30,\n",
    "\n",
    "    # diffusion hyperparameters\n",
    "    timesteps = 500,\n",
    "    beta1 = 1e-4,\n",
    "    beta2 = 0.02,\n",
    "\n",
    "    # network hyperparameters\n",
    "    n_feat = 64, # 64 hidden dimension feature\n",
    "    n_cfeat = 5, # context vector is of size 5\n",
    "    height = 16, # 16x16 image\n",
    "    \n",
    "    # training hyperparameters\n",
    "    batch_size = 100,\n",
    "    n_epoch = 32,\n",
    "    lrate = 1e-3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup DDPM noise scheduler and sampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ddpm sampler functions\n",
    "# - perturb_input: Adds noise to the input image at the corresponding timestep on the schedule\n",
    "# - sample_ddpm_context: Generate images using the DDPM sampler, we will use this function during \n",
    "#                        training to sample from the model regularly and see how our training is progressing\n",
    "perturb_input, sample_ddpm_context = setup_ddpm(config.beta1, \n",
    "                                                config.beta2, \n",
    "                                                config.timesteps, \n",
    "                                                DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "nn_model = ContextUnet(in_channels=3, \n",
    "                       n_feat=config.n_feat, \n",
    "                       n_cfeat=config.n_cfeat, \n",
    "                       height=config.height).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset and construct optimizer\n",
    "dataset = CustomDataset.from_np(path=DATA_DIR)\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=config.batch_size, \n",
    "                        shuffle=True)\n",
    "optim = torch.optim.Adam(nn_model.parameters(), lr=config.lrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We use a fixed context vector with 6 samples of each class to guide our diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise vector\n",
    "# x_T ~ N(0, 1), sample initial noise\n",
    "# - we choose noise only once for consistency\n",
    "noises = torch.randn(config.num_samples, 3, \n",
    "                     config.height, config.height).to(DEVICE)  \n",
    "\n",
    "# A fixed context vector to sample from\n",
    "ctx_vector = F.one_hot(torch.tensor([0,0,0,0,0,0,   # hero\n",
    "                                     1,1,1,1,1,1,   # non-hero\n",
    "                                     2,2,2,2,2,2,   # food\n",
    "                                     3,3,3,3,3,3,   # spell\n",
    "                                     4,4,4,4,4,4]), # side-facing \n",
    "                       5).to(DEVICE).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33md-oliver-cort\u001b[0m (\u001b[33mdoc93\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/davidoc/Documents/personal/diffusion-models/notebooks/wandb/run-20230814_190451-vc3xwhf5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/doc93/diff_model_sprite/runs/vc3xwhf5' target=\"_blank\">radiant-dew-1</a></strong> to <a href='https://wandb.ai/doc93/diff_model_sprite' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/doc93/diff_model_sprite' target=\"_blank\">https://wandb.ai/doc93/diff_model_sprite</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/doc93/diff_model_sprite/runs/vc3xwhf5' target=\"_blank\">https://wandb.ai/doc93/diff_model_sprite/runs/vc3xwhf5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [1:06:42<7:51:35, 1010.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Create a wandb run - to track training\n",
    "# - job_type: helps to identify this job later on\n",
    "run = wandb.init(project=\"diff_model_sprite\", \n",
    "                 job_type=\"train\",   \n",
    "                 config=config)\n",
    "\n",
    "# Pass the config back from W&B\n",
    "config = wandb.config\n",
    "\n",
    "for ep in tqdm(range(config.n_epoch), leave=True, total=config.n_epoch):\n",
    "    # Set model to train mode\n",
    "    nn_model.train()\n",
    "    optim.param_groups[0]['lr'] = config.lrate*(1-ep/config.n_epoch)\n",
    "    \n",
    "    pbar = tqdm(dataloader, leave=False)\n",
    "    for x, c in pbar:   # x: images  c: context\n",
    "        # Set gradients to zero for every mini-batch training iteration\n",
    "        optim.zero_grad()\n",
    "        x = x.to(DEVICE)\n",
    "        c = c.to(DEVICE)   \n",
    "\n",
    "        # Randomly mask out c\n",
    "        context_mask = torch.bernoulli(torch.zeros(c.shape[0]) + 0.8).to(DEVICE)\n",
    "        # - with some randomness, we completely mask out the context so that \n",
    "        #   the model is able to generally learn what a sprite is as well\n",
    "        c = c * context_mask.unsqueeze(-1)   \n",
    "\n",
    "        # Perturb data\n",
    "        # - perturb_input: Adds noise to the input image at the corresponding timestep on the schedule\n",
    "        noise = torch.randn_like(x)\n",
    "        t = torch.randint(1, config.timesteps + 1, (x.shape[0],)).to(DEVICE) \n",
    "        x_pert = perturb_input(x, t, noise)  \n",
    "\n",
    "        # Use network to recover noise    \n",
    "        pred_noise = nn_model(x_pert, t / config.timesteps, c=c)   \n",
    "\n",
    "        # Loss: mean squared error between the predicted and true noise   \n",
    "        loss = F.mse_loss(pred_noise, noise)\n",
    "\n",
    "        # Back propagation to compute gradients\n",
    "        loss.backward()    \n",
    "        # Update weights\n",
    "        optim.step()\n",
    "\n",
    "        # Log parameters at the end of each mini-batch iteration\n",
    "        wandb.log({\"loss\": loss.item(),\n",
    "                   \"lr\": optim.param_groups[0]['lr'],\n",
    "                   \"epoch\": ep})\n",
    "\n",
    "\n",
    "    # save model every 4 epochs\n",
    "    if ep%4==0 or ep == int(config.n_epoch-1):\n",
    "        nn_model.eval()\n",
    "        ckpt_file = SAVE_DIR/f\"context_model.pth\"\n",
    "        torch.save(nn_model.state_dict(), ckpt_file)\n",
    "\n",
    "        # Create W&B artifact - to record model weights\n",
    "        artifact_name = f\"{wandb.run.id}_context_model\"\n",
    "        at = wandb.Artifact(artifact_name, type=\"model\")\n",
    "        at.add_file(ckpt_file)\n",
    "        wandb.log_artifact(at, aliases=[f\"epoch_{ep}\"])\n",
    "\n",
    "        # sample_ddpm_context: Generate images using the DDPM sampler\n",
    "        samples, _ = sample_ddpm_context(nn_model, \n",
    "                                         noises, \n",
    "                                         ctx_vector[:config.num_samples])\n",
    "        \n",
    "        # Sample images to view in W&B workspace\n",
    "        wandb.log({\n",
    "            \"train_samples\": [\n",
    "                wandb.Image(img) for img in samples.split(1)\n",
    "            ]})\n",
    "        \n",
    "# finish W&B run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_diffusion_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
